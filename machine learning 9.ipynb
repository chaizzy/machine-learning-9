{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11362ad2-2a04-4c90-81e7-12d387668a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1:\n",
    "# \"Linear regression\"\n",
    "# 1. It comes under regression algorithm  where output feature is continous (numerical)\n",
    "# 2. main aim for regression module is to find the best fit line for prediction\n",
    "# 3. Linear regression is used to model the relationship between a dependent variable and one or more independent variables. \n",
    "# for example \n",
    "# A Dataset consisting of weight and height feature \n",
    "# where weight is independent feature and height is dependent feature\n",
    "# model should predict the height when we give weight \n",
    "\n",
    "\n",
    "# \"logistic regression\"\n",
    "# 1.It comes under classification algorithm where output feature is categorical feature\n",
    "# 2. main aim is to solve classification problem \n",
    "# 3.It models the probability of an event occurring based on the values of independent variables. \n",
    "# 4 The dependent variable in logistic regression is typically binary (e.g., yes/no, true/false,0/1), representing two possible outcomes.\n",
    "# for example\n",
    "# Dataset containing features like no. of play hours , Result(pass/fail)\n",
    "# we have to  predict  the student's result by giving no.of play hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a42c188-8485-4604-b79b-8b97290a4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2 :\n",
    "# The cost function used in logistic regression is called the \"logistic loss\" or \"log loss\" function. \n",
    "# It measures the error between the predicted probabilities and the actual binary outcomes in the training data.\n",
    "# Cost(y, y_hat) = -[y * log(y_hat) + (1 - y) * log(1 - y_hat)]\n",
    "# The logistic loss function penalizes incorrect predictions. \n",
    "\n",
    "# The optimization of logistic regression involves finding the coefficients that minimize the overall logistic loss \n",
    "# across all training examples,\n",
    "# Squashing the best fit line by sigmoid activation function\n",
    "# sigma = 1 / 1 + e(pov(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93358c80-6567-42a6-9494-7ecb9181f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3 :\n",
    "# Regularization in logistic regression is a technique used to prevent overfitting,\n",
    "# Regularization adds a penalty term to the cost function, encouraging the model to find simpler and more generalizable solutions.\n",
    "\n",
    "# There are two 3 regularization techniques\n",
    "# 1.L1 Regularization (Lasso): L1 regularization adds a penalty term to the cost function that is proportional to the sum of the \n",
    "#     absolute values of the coefficients\n",
    "# 2.L2 Regularization (Ridge): L2 regularization adds a penalty term to the cost function that is proportional to the sum of the \n",
    "#    squared values of the coefficients.\n",
    "# 3. Elastic Net regression is a linear regression model that combines both L1 (Lasso) and L2 (Ridge) regularization techniques.\n",
    "\n",
    "# It prevents overfitting\n",
    "#  regularization in logistic regression adds a penalty term to the cost function,promoting simpler models with smaller coefficients. \n",
    "# This helps prevent overfitting by reducing the impact of noisy features and encouraging generalization to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9336423f-9947-4857-859d-532a68e08a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4 :\n",
    "# \"ROC CURVE\"\n",
    "# The Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a binary classification model, \n",
    "# such as logistic regression, at different classification thresholds. \n",
    "# It plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) as the classification threshold varies.\n",
    "\n",
    "# It evaluates the performance of logistic regression\n",
    "# 1.Prediction and Threshold: For each example in the test dataset, the logistic regression model predicts \n",
    "#   the probability of belonging to the positive class. \n",
    "#   By varying the classification threshold (usually between 0 and 1), \n",
    "#   you can control the balance between true positives and false positives.\n",
    "# 2.Calculation of True Positive Rate and False Positive Rate: With different classification thresholds, \n",
    "#   the true positive rate (TPR) and false positive rate (FPR) are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b74361a-b803-40fa-999e-5383b5b450c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5 :\n",
    "# 1.Univariate Selection:Features with high statistical significance are selected for the model. \n",
    "#   This technique is simple and fast but ignores interactions between features.\n",
    "# 2.L1 Regularization (Lasso): L1 regularization in logistic regression encourages sparsity by driving some coefficients to zero. \n",
    "#   Features with non-zero coefficients are considered important and selected for the model.\n",
    "\n",
    "# It improves the performance\n",
    "# 1.Improved Model Performance:The resulting model focuses on the most informative features, \n",
    "#    leading to improved performance in terms of accuracy, precision, recall, or other evaluation metrics.\n",
    "# 2.Reduced Overfitting:This reduces the risk of overfitting, where the model becomes too specific to the training data and \n",
    "#   performs poorly on new, unseen data.\n",
    "# 3.Enhanced Interpretability: A model with a smaller set of selected features is more interpretable and easier to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fcafc43-be14-49a5-8727-7c2f44402a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6:\n",
    "# \"Handling Imbalnced Data\"\n",
    "# 1.Resampling Techniques: Resampling techniques involve either oversampling the minority class or \n",
    "#   undersampling the majority class to create a more balanced dataset.\n",
    "#  ---Oversampling: Oversampling techniques increase the number of instances in the minority class by duplicating or \n",
    "#                   generating synthetic examples. \n",
    "#                   This can be done using methods like Random Oversampling, SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "# ----undersampling: Undersampling techniques reduce the number of instances in the majority class by randomly removing examples.\n",
    "# 2.Collect More Data: Increasing the amount of data for the minority class can help improve the model's ability to learn from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc7479a-f45e-4405-986d-70c46265c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 7 :\n",
    "# There are some issues:\n",
    "# 1.Multicollinearity among Independent Variables: Multicollinearity occurs when there is a high correlation between independent variables. \n",
    "# ----  Remove or combine correlated variables: Remove one of the highly correlated variables or combine them into a single variable.\n",
    "# 2.Missing Data: Missing data can lead to biased or inefficient parameter estimates. \n",
    "#             Several strategies can be employed to handle missing data:\n",
    "# ---------Imputation: Fill in missing values using techniques like mean imputation, median imputation, or regression imputation.\n",
    "# 3.Outliers: Outliers can have a significant impact on the logistic regression model, leading to biased coefficient estimates. \n",
    "#               Strategies to handle outliers include:\n",
    "#------Identification: Detect outliers using statistical techniques such as z-scores, box plots.\n",
    "# 4.Model Overfitting: Overfitting occurs when the model fits the training data too closely but performs poorly on new data. \n",
    "#                      To address overfitting:\n",
    "#---------------Regularization: Apply L1 or L2 regularization to shrink coefficients and prevent overemphasis on specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7990bea-1e0c-4565-a929-e007a357315f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
